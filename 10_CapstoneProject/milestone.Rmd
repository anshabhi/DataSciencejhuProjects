---
title: "Milestone Report"
author: "Abhinav Agarwal"
date: "8 January 2019"
output: html_document
---


```{r setup, include=FALSE, warning=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This milestone report is based on exploratory data analysis of the SwifKey data provided in the context of the Coursera Data Science Capstone. 


## Getting The Data
It is assumed that the data has been downloaded, unzipped and placed into the active R directory, maintaining the folder structure.

```{r}
library(readtext)
library(quanteda)
blogs <- readtext("en_US.blogs.txt")
news <- readtext("en_US.news.txt")
twitter <- readtext("en_US.twitter.txt")
```
We examine the data sets and summarize our findings (file sizes, line counts, word counts, and mean words per line) below.
```{r}

# Get file sizes
blogs.size <- file.info("en_US.blogs.txt")$size / 1024 ^ 2
news.size <- file.info("en_US.news.txt")$size / 1024 ^ 2
twitter.size <- file.info("en_US.twitter.txt")$size / 1024 ^ 2

#Get line tokens
blogs.s <- tokens(corpus(blogs), what = "sentence")
news.s <- tokens(corpus(news), what = "sentence")  
twitter.s <- tokens(corpus(twitter), what = "sentence") 

# Make word tokens
blogs.t <- (tokens(corpus(blogs)))
news.t <- (tokens(corpus(news)))
twitter.t <-(tokens(corpus(twitter)))

# Get words in files
blogs.words <- ntoken(blogs.t)
news.words <- ntoken(news.t)
twitter.words <- ntoken(twitter.t)

#Get lines in files
blogs.lines <- ntoken(blogs.s)
news.lines <- ntoken(news.s)
twitter.lines <- ntoken(twitter.s)
 # Summary of the data sets
data.frame(source = c("blogs", "news", "twitter"),
           file.size.MB = c(blogs.size, news.size, twitter.size),
           num.lines = c(blogs.lines,news.lines , twitter.lines),
           num.words = c(blogs.words, news.words, twitter.words),
           mean.num.words = c((blogs.words)/ntoken(blogs.s), news.words/ntoken(news.s), (twitter.words)/ntoken(twitter.s)))
```

## Cleaning The Data
We now clean all the three dfms that we have, removing the not required elements like numbers, punctuations,symbols  etc and stopwords
```{r}
blogs.d <- dfm(blogs.t,tolower=TRUE,remove_numbers = TRUE, remove_punct = TRUE, remove_twitter = TRUE, remove_symbols = TRUE, remove_hyphens = TRUE,remove = stopwords("english"))

news.d <- dfm(news.t,tolower=TRUE,remove_numbers = TRUE, remove_punct = TRUE,remove_twitter = TRUE, remove_symbols = TRUE,remove_hyphens = TRUE,remove = stopwords("english"))

twitter.d <- dfm(twitter.t,tolower=TRUE,remove_numbers = TRUE, remove_punct = TRUE, remove_twitter = TRUE, remove_symbols = TRUE,remove_hyphens = TRUE,remove = stopwords("english"))
```

## Exploratory Data Analysis 

We will now create word clouds to explore some of the most common words.
We first create a bigDFM, consisting of elements of all other 3 DFMs.

```{r}
bigDFM <- cbind(blogs.d,news.d,twitter.d)

#Compressing as cbind might result in duplicates in the dfm 

bigDFM<-dfm_compress(bigDFM)

textplot_wordcloud(bigDFM,random_color = TRUE,color = c("Violet","Red","Green","Orange"))
```

The following is the list of top 20 most frequent words in the 3 datasets, with their frequencies:

```{r}
topfeatures(bigDFM,20)
```
## Conclusion

This was an introduction to the prediction model that we are going to develop, and finally publishing using Shiny apps.
This document also demonstrates the incredible power of quantmod library. This document was compiled on an HP Probook G4, with i5 4th gen processor and 8 GB RAM.